import os
import json
import time
import datetime
import requests
import feedparser
import html2text
import schedule
import xml.etree.ElementTree as ET
import hmac
import hashlib
import base64
import urllib.parse
from urllib.parse import urlparse
from podcast_analyzer import analyze_podcast_audio

# ==========================================
# é…ç½®
# ==========================================
# åŠ è½½é…ç½®æ–‡ä»¶
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_FILE = os.path.join(CURRENT_DIR, "config.json")

try:
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        config = json.load(f)
except Exception as e:
    print(f"[-] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    config = {}

# API Key é…ç½®
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", config.get("deepseek_api_key", ""))
OPENAI_BASE_URL = config.get("deepseek_base_url", "https://api.deepseek.com")
MODEL_NAME = config.get("deepseek_model", "deepseek-chat")
TIME_WINDOW_HOURS = config.get("time_window_hours", 24)
LIMIT_TESTING = config.get("limit_testing", False)

# æ–‡ä»¶è·¯å¾„é…ç½®
files_config = config.get("files", {})
RSS_MAP_FILE = os.path.join(CURRENT_DIR, files_config.get("rss_map_file", "known_rss_map.json"))
SOURCE_FILE = os.path.join(CURRENT_DIR, files_config.get("source_file", "channels_from_excel.json"))
PODCAST_OPML_FILE = os.path.join(CURRENT_DIR, files_config.get("podcast_opml_file", "../BestBlogs_RSS_Podcasts.opml"))
OUTPUT_DIR = os.path.join(CURRENT_DIR, files_config.get("output_dir", "daily_reports"))

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# DingTalk é…ç½®
DINGTALK_CONFIG = config.get("dingtalk", {})
DINGTALK_WEBHOOK = os.environ.get("DINGTALK_WEBHOOK", DINGTALK_CONFIG.get("webhook_url", ""))
DINGTALK_SECRET = os.environ.get("DINGTALK_SECRET", DINGTALK_CONFIG.get("secret", ""))


# æ ¸å¿ƒ Prompt
ARTICLE_ANALYSIS_PROMPT = """
# æ·±åº¦æ–‡ç« åˆ†æä¸“å®¶

## è§’è‰²
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šåˆ†æå¸ˆï¼Œæ“…é•¿ä»é•¿æ–‡ä¸­æç‚¼é«˜ä»·å€¼ä¿¡æ¯ã€‚

## ç›®æ ‡
æ·±åº¦é˜…è¯»æ–‡ç« å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½åŒ…å«å…³é”®ç»†èŠ‚çš„è¯¦ç»†æ‘˜è¦ã€‚**æ‹’ç»ç©ºæ´çš„å¥—è¯ï¼Œå¿…é¡»ä¿ç•™å…·ä½“çš„è®ºæ®ã€æ•°æ®å’Œäº‹å®ã€‚**

## åˆ†æè¦æ±‚
1. **è¯¦ç»†æ‘˜è¦ (summary)**:
   - å­—æ•°è¦æ±‚: 300-600å­—ã€‚
   - å†…å®¹è¦æ±‚: å¿…é¡»æ¶µç›–æ–‡ç« çš„æ ¸å¿ƒè®ºç‚¹ã€æ”¯æŒè¿™äº›è®ºç‚¹çš„å…³é”®è®ºæ®ã€å¼•ç”¨çš„å…·ä½“æ•°æ®æˆ–æ¡ˆä¾‹ã€ä»¥åŠé‡è¦çš„äº‹å®é™ˆè¿°ã€‚
   - é£æ ¼è¦æ±‚: é€»è¾‘æ¸…æ™°ï¼Œä¿¡æ¯å¯†åº¦é«˜ï¼Œè®©è¯»è€…ä¸çœ‹åŸæ–‡ä¹Ÿèƒ½è·å– 90% çš„å…³é”®ä¿¡æ¯ã€‚
2. **ä¸€å¥è¯æ€»ç»“ (one_sentence_summary)**: 50å­—ä»¥å†…ï¼Œé«˜åº¦æ¦‚æ‹¬ã€‚
3. **å…³é”®æ´å¯Ÿ (key_takeaways)**: 3-5 ä¸ªå…·ä½“çš„æ·±åº¦æ´å¯Ÿã€‚

## è¾“å‡ºæ ¼å¼ (JSON)
è¯·ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼Œç¡®ä¿ JSON æ ¼å¼åˆæ³•ï¼š
{
  "title_translated": "ä¸­æ–‡æ ‡é¢˜",
  "one_sentence_summary": "ä¸€å¥è¯æ ¸å¿ƒæ€»ç»“",
  "summary": "è¯¦ç»†æ‘˜è¦(åŒ…å«è§‚ç‚¹ã€è®ºæ®ã€æ•°æ®ã€äº‹å®)",
  "key_takeaways": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2", "å…³é”®æ´å¯Ÿ3"],
  "domain": "æ‰€å±é¢†åŸŸ",
  "score": 85,
  "reason": "è¯„åˆ†ç†ç”±"
}
"""

# ==========================================
# å·¥å…·å‡½æ•°
# ==========================================

def load_rss_feeds():
    """
    åŠ è½½ RSS æºåˆ—è¡¨ã€‚
    ä¼˜å…ˆä½¿ç”¨ known_rss_map.json ä¸­çš„æ˜ å°„ã€‚
    """
    feeds = []
    
    # 1. åŠ è½½æ˜ å°„è¡¨
    rss_map = {}
    if os.path.exists(RSS_MAP_FILE):
        with open(RSS_MAP_FILE, 'r', encoding='utf-8') as f:
            rss_map = json.load(f)
            
    # 2. åŠ è½½æºæ–‡ä»¶ (ä¸ºäº†è·å–ç½‘ç«™åç§°ç­‰å…ƒæ•°æ®)
    if os.path.exists(SOURCE_FILE):
        with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
            sources = json.load(f)
            
        for item in sources:
            url = item.get("ç½‘å€")
            name = item.get("å§“å")
            
            # å¦‚æœè¯¥ç½‘å€æœ‰å·²çŸ¥çš„ RSS
            if url in rss_map:
                feeds.append({
                    "name": name,
                    "homepage": url,
                    "rss_url": rss_map[url]
                })
            # å¦‚æœ URL æœ¬èº«çœ‹èµ·æ¥åƒ RSS (è™½ç„¶æºæ–‡ä»¶é‡Œå¤§éƒ¨åˆ†æ˜¯ä¸»é¡µ)
            elif url.endswith('.xml') or url.endswith('/feed'):
                 feeds.append({
                    "name": name,
                    "homepage": url,
                    "rss_url": url
                })
    
    print(f"[*] å·²åŠ è½½ {len(feeds)} ä¸ªæœ‰æ•ˆçš„ RSS è®¢é˜…æº")
    return feeds

def load_opml_feeds(file_path, limit=None):
    """
    ä» OPML æ–‡ä»¶åŠ è½½æ’­å®¢æº
    """
    feeds = []
    if not os.path.exists(file_path):
        print(f"[-] OPML æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return feeds
        
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        # æŸ¥æ‰¾æ‰€æœ‰ type="rss" çš„ outline
        for outline in root.findall(".//outline[@type='rss']"):
            title = outline.get("text") or outline.get("title")
            xml_url = outline.get("xmlUrl")
            
            if title and xml_url:
                feeds.append({
                    "name": title,
                    "homepage": xml_url, # æ’­å®¢é€šå¸¸æ²¡æœ‰å•ç‹¬çš„ä¸»é¡µ URL å­—æ®µï¼Œæš‚æ—¶ç”¨ rss url ä»£æ›¿æˆ–ç•™ç©º
                    "rss_url": xml_url,
                    "is_podcast": True
                })
                
    except Exception as e:
        print(f"[-] è§£æ OPML å¤±è´¥: {e}")
        
    print(f"[*] å·²åŠ è½½ {len(feeds)} ä¸ªæ’­å®¢æº")
    
    if limit:
        print(f"[*] é™åˆ¶æµ‹è¯•: ä»…ä¿ç•™å‰ {limit} ä¸ªæ’­å®¢æº")
        feeds = feeds[:limit]
        
    return feeds

def fetch_url_content(url):
    """è·å– URL å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        return resp.content
    except Exception as e:
        print(f"[-] è¯·æ±‚å¤±è´¥ {url}: {e}")
        return None

def html_to_markdown(html_content):
    """HTML è½¬ Markdown"""
    if not html_content:
        return ""
    
    try:
        html_text = html_content.decode('utf-8')
    except:
        try:
            html_text = html_content.decode('gbk')
        except:
            html_text = html_content.decode('utf-8', errors='ignore')

    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = True
    h.body_width = 0
    return h.handle(html_text)

def call_deepseek_analyze(content):
    """è°ƒç”¨ DeepSeek è¿›è¡Œåˆ†æ"""
    if len(content) > 10000:
        content = content[:10000] + "...(truncated)"
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}"
        }
        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": ARTICLE_ANALYSIS_PROMPT},
                {"role": "user", "content": content}
            ],
            "temperature": 0.5,
            "stream": False
        }
        resp = requests.post(f"{OPENAI_BASE_URL}/chat/completions", json=payload, headers=headers, timeout=60)
        
        if resp.status_code != 200:
            print(f"[-] LLM API Error: {resp.text}")
            return None
            
        result = resp.json()['choices'][0]['message']['content']
        # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
        result = result.replace('```json', '').replace('```', '').strip()
        return json.loads(result)
    except Exception as e:
        print(f"[-] LLM åˆ†æå¤±è´¥: {e}")
        return None

def send_dingtalk_notification(title, text):
    """å‘é€é’‰é’‰æœºå™¨äººé€šçŸ¥ (æ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µ)"""
    if not DINGTALK_WEBHOOK:
        print("[-] æœªé…ç½®é’‰é’‰ Webhookï¼Œè·³è¿‡å‘é€ã€‚")
        return

    webhook_url = DINGTALK_WEBHOOK
    
    # å¦‚æœé…ç½®äº†åŠ ç­¾ (Secret)
    if DINGTALK_SECRET:
        timestamp = str(round(time.time() * 1000))
        secret_enc = DINGTALK_SECRET.encode('utf-8')
        string_to_sign = '{}\n{}'.format(timestamp, DINGTALK_SECRET)
        string_to_sign_enc = string_to_sign.encode('utf-8')
        hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()
        sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
        webhook_url = f"{DINGTALK_WEBHOOK}&timestamp={timestamp}&sign={sign}"

    # åˆ†æ®µå‘é€é€»è¾‘
    # é’‰é’‰é™åˆ¶æ¯ä¸ªæ¶ˆæ¯å¤§æ¦‚ 20000 å­—èŠ‚ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œé™åˆ¶åœ¨ 4000 å­—ç¬¦å·¦å³åˆ†æ®µ
    MAX_LENGTH = 4000
    
    # ç®€å•çš„æŒ‰é•¿åº¦åˆ‡åˆ†å¯èƒ½ä¼šåˆ‡æ–­ Markdown æ ¼å¼ï¼Œå°è¯•æŒ‰è¡Œåˆ‡åˆ†
    lines = text.split('\n')
    chunks = []
    current_chunk = ""
    
    for line in lines:
        if len(current_chunk) + len(line) + 1 > MAX_LENGTH:
            chunks.append(current_chunk)
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"
            
    if current_chunk:
        chunks.append(current_chunk)
        
    print(f"[*] æ¶ˆæ¯è¿‡é•¿ï¼Œå·²åˆ‡åˆ†ä¸º {len(chunks)} æ¡å‘é€")

    for i, chunk in enumerate(chunks):
        # æ„é€ æ¶ˆæ¯
        # é’‰é’‰ Markdown æ¶ˆæ¯
        chunk_title = title if i == 0 else f"{title} (Part {i+1})"
        data = {
            "msgtype": "markdown",
            "markdown": {
                "title": chunk_title,
                "text": chunk
            }
        }

        try:
            resp = requests.post(webhook_url, json=data)
            if resp.json().get("errcode") == 0:
                print(f"[+] é’‰é’‰é€šçŸ¥ (Part {i+1}) å‘é€æˆåŠŸ")
            else:
                print(f"[-] é’‰é’‰é€šçŸ¥ (Part {i+1}) å‘é€å¤±è´¥: {resp.text}")
            
            # ç¨å¾®å»¶æ—¶é¿å…è§¦å‘é¢‘ç‡é™åˆ¶
            time.sleep(1)
            
        except Exception as e:
            print(f"[-] å‘é€é’‰é’‰è¯·æ±‚å¼‚å¸¸: {e}")

def process_feed(feed):
    """å¤„ç†å•ä¸ª RSS Feed"""
    print(f"[*] æ­£åœ¨æ£€æŸ¥: {feed['name']} ({feed['rss_url']})")
    
    try:
        # è§£æ RSS
        d = feedparser.parse(feed['rss_url'])
        
        today_articles = []
        # å®šä¹‰ "ä»Šå¤©" çš„èŒƒå›´ (è¿‡å» 24 å°æ—¶)
        now = datetime.datetime.now()
        
        for entry in d.entries:
            # è·å–å‘å¸ƒæ—¶é—´
            published_time = None
            if hasattr(entry, 'published_parsed'):
                published_time = datetime.datetime.fromtimestamp(time.mktime(entry.published_parsed))
            elif hasattr(entry, 'updated_parsed'):
                published_time = datetime.datetime.fromtimestamp(time.mktime(entry.updated_parsed))
            
            # å¦‚æœæ²¡æœ‰æ—¶é—´ï¼Œæˆ–è€…æ—¶é—´åœ¨ 24 å°æ—¶å†…
            is_new = False
            if published_time:
                # ç®€å•åˆ¤æ–­ï¼šè¿‡å» TIME_WINDOW_HOURS å°æ—¶
                if (now - published_time).total_seconds() < TIME_WINDOW_HOURS * 3600:
                    is_new = True
            else:
                pass 
            
            if is_new:
                print(f"  [+] å‘ç°æ–°å†…å®¹: {entry.title}")
                link = entry.link
                analysis = None
                is_podcast_entry = False

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ’­å®¢ (Audio Enclosure)
                audio_url = None
                if hasattr(entry, 'enclosures'):
                    for enclosure in entry.enclosures:
                        if enclosure.type and enclosure.type.startswith('audio/'):
                            audio_url = enclosure.href
                            break
                
                # å¦‚æœæ˜¯æ’­å®¢æºæˆ–è€…æ˜¯éŸ³é¢‘å†…å®¹
                if audio_url:
                    is_podcast_entry = True
                    print(f"   [ğŸ™ï¸] è¯†åˆ«ä¸ºæ’­å®¢éŸ³é¢‘: {audio_url}")
                    analysis = analyze_podcast_audio(audio_url)
                else:
                    # æ™®é€šæ–‡ç« 
                    content_html = fetch_url_content(link)
                    content_md = html_to_markdown(content_html)
                    if content_md:
                         analysis = call_deepseek_analyze(content_md)

                if analysis:
                    today_articles.append({
                        "original_title": entry.title,
                        "link": link,
                        "author": feed['name'],
                        "published": published_time.strftime("%Y-%m-%d %H:%M") if published_time else "Unknown",
                        "analysis": analysis,
                        "is_podcast": is_podcast_entry
                    })
            else:
                if published_time:
                    print(f"  [-] è·³è¿‡æ—§å†…å®¹: {entry.title} ({published_time})")
                else:
                    print(f"  [-] è·³è¿‡æ— æ—¶é—´æˆ³å†…å®¹: {entry.title}")
                        
        return today_articles
        
    except Exception as e:
        print(f"[-] å¤„ç† Feed å¤±è´¥ {feed['rss_url']}: {e}")
        return []

def generate_daily_report(articles):
    """ç”Ÿæˆæ—¥æŠ¥ Markdown"""
    if not articles:
        print("[!] ä»Šå¤©æ²¡æœ‰æ–°æ–‡ç« ï¼Œä¸ç”ŸæˆæŠ¥å‘Šã€‚")
        return
    
    date_str = datetime.datetime.now().strftime("%Y-%m-%d")
    filename = f"Daily_Digest_{date_str}.md"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# ğŸ“… Daily RSS Digest - {date_str}\n\n")
        f.write(f"> ä»Šæ—¥å…±æ›´æ–° {len(articles)} ç¯‡æ–‡ç« \n\n")
        f.write("---\n\n")
        
        for i, article in enumerate(articles, 1):
            analysis = article['analysis']
            title_prefix = "[ğŸ™ï¸ æ’­å®¢] " if article.get('is_podcast') else ""
            f.write(f"## {i}. {title_prefix}{analysis.get('title_translated', article['original_title'])}\n\n")
            f.write(f"- **æ¥æº**: {article['author']}\n")
            f.write(f"- **å‘å¸ƒæ—¶é—´**: {article['published']}\n")
            f.write(f"- **åŸæ–‡é“¾æ¥**: [ç‚¹å‡»é˜…è¯»]({article['link']})\n")
            f.write(f"- **é¢†åŸŸ**: `{analysis.get('domain', 'æœªçŸ¥')}`\n")
            f.write(f"- **è¯„åˆ†**: {analysis.get('score', 0)} / 100\n\n")
            
            f.write(f"### ğŸ“ æ ¸å¿ƒæ‘˜è¦\n")
            f.write(f"> **{analysis.get('one_sentence_summary', '')}**\n\n")
            f.write(f"{analysis.get('summary', '')}\n\n")
            
            f.write(f"### ğŸ’¡ å…³é”®æ´å¯Ÿ\n")
            for point in analysis.get('key_takeaways', []):
                f.write(f"- {point}\n")
            
            f.write(f"\n> *è¯„åˆ†ç†ç”±: {analysis.get('reason', '')}*\n\n")
            f.write("---\n\n")
            
    print(f"\n[âˆš] æ—¥æŠ¥å·²ç”Ÿæˆ: {filepath}")
    
    # è¯»å–ç”Ÿæˆçš„æ–‡ä»¶å†…å®¹ç”¨äºå‘é€
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
        
    # å‘é€é’‰é’‰é€šçŸ¥
    # é’‰é’‰æœ‰æ¶ˆæ¯é•¿åº¦é™åˆ¶ï¼Œè¿™é‡Œåšä¸ªç®€å•æˆªæ–­ä¿æŠ¤ï¼Œæˆ–è€…ä»…å‘é€æ‘˜è¦é“¾æ¥ï¼ˆå¦‚æœæœ‰åœ¨çº¿ç‰ˆï¼‰
    # ç›®å‰æˆ‘ä»¬å‘é€å…¨é‡ï¼Œå¦‚æœè¿‡é•¿å¯èƒ½éœ€è¦åˆ‡å‰²
    if content:
        send_dingtalk_notification(f"RSS Daily Digest {date_str}", content)
        
    return filepath

def job():
    print(f"\n[{datetime.datetime.now()}] å¼€å§‹æ‰§è¡Œæ¯æ—¥ä»»åŠ¡...")
    
    # ç¡®å®šé™åˆ¶æ•°é‡
    limit_count = None
    if LIMIT_TESTING:
        # å¦‚æœæ˜¯ Trueï¼Œé»˜è®¤é™åˆ¶ä¸º 1ï¼›å¦‚æœæ˜¯æ•°å­—ï¼Œåˆ™ä½¿ç”¨è¯¥æ•°å­—
        limit_count = 1 if isinstance(LIMIT_TESTING, bool) else int(LIMIT_TESTING)
        print(f"[*] æµ‹è¯•æ¨¡å¼å¼€å¯: ä»…å¤„ç†å‰ {limit_count} ä¸ªæº")

    # 1. åŠ è½½æ–‡ç«  RSS
    feeds = load_rss_feeds()
    if limit_count:
        feeds = feeds[:limit_count]
    
    # 2. åŠ è½½æ’­å®¢ RSS
    podcast_feeds = load_opml_feeds(PODCAST_OPML_FILE, limit=limit_count)
    feeds.extend(podcast_feeds)
    
    all_articles = []
    
    for feed in feeds:
        articles = process_feed(feed)
        all_articles.extend(articles)
        
    generate_daily_report(all_articles)
    print(f"[{datetime.datetime.now()}] ä»»åŠ¡å®Œæˆã€‚\n")

if __name__ == "__main__":
    print("Daily Digest Service Started...")
    
    # ç«‹å³è¿è¡Œä¸€æ¬¡æµ‹è¯•
    job()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡ (ä¾‹å¦‚æ¯å¤©æ—©ä¸Š 08:00)
    # schedule.every().day.at("08:00").do(job)
    
    # while True:
    #     schedule.run_pending()
    #     time.sleep(60)
